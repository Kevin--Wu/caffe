I1122 10:35:16.974933  2348 caffe.cpp:217] Using GPUs 0
I1122 10:35:17.127135  2348 caffe.cpp:222] GPU 0: Tesla K40c
I1122 10:35:17.669770  2348 solver.cpp:48] Initializing solver from parameters: 
test_iter: 300
test_interval: 500
base_lr: 0.01
display: 500
max_iter: 100000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 10000
snapshot: 10000
snapshot_prefix: "mywork/ucf101/snapshot/ucf_vgg2048"
solver_mode: GPU
device_id: 0
net: "mywork/ucf101/vgg_m_2048_net.prototxt"
train_state {
  level: 0
  stage: ""
}
type: "SGD"
I1122 10:35:17.681232  2348 solver.cpp:91] Creating training net from net file: mywork/ucf101/vgg_m_2048_net.prototxt
I1122 10:35:17.681769  2348 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1122 10:35:17.681988  2348 net.cpp:58] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 224
    mean_file: "mywork/ucf101/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/home/hadoop/whx/dataset/ucf101/jpgoutput/split1/ucf101_train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 101
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1122 10:35:17.682102  2348 layer_factory.hpp:77] Creating layer data
I1122 10:35:17.682529  2348 net.cpp:100] Creating Layer data
I1122 10:35:17.682543  2348 net.cpp:408] data -> data
I1122 10:35:17.682559  2348 net.cpp:408] data -> label
I1122 10:35:17.682569  2348 data_transformer.cpp:25] Loading mean file from: mywork/ucf101/imagenet_mean.binaryproto
I1122 10:35:17.992344  2366 db_lmdb.cpp:35] Opened lmdb /home/hadoop/whx/dataset/ucf101/jpgoutput/split1/ucf101_train_lmdb
I1122 10:35:18.472220  2348 data_layer.cpp:41] output data size: 128,3,224,224
I1122 10:35:18.570581  2348 net.cpp:150] Setting up data
I1122 10:35:18.570611  2348 net.cpp:157] Top shape: 128 3 224 224 (19267584)
I1122 10:35:18.570617  2348 net.cpp:157] Top shape: 128 (128)
I1122 10:35:18.570621  2348 net.cpp:165] Memory required for data: 77070848
I1122 10:35:18.570628  2348 layer_factory.hpp:77] Creating layer label_data_1_split
I1122 10:35:18.570637  2348 net.cpp:100] Creating Layer label_data_1_split
I1122 10:35:18.570642  2348 net.cpp:434] label_data_1_split <- label
I1122 10:35:18.570652  2348 net.cpp:408] label_data_1_split -> label_data_1_split_0
I1122 10:35:18.570659  2348 net.cpp:408] label_data_1_split -> label_data_1_split_1
I1122 10:35:18.570683  2348 net.cpp:150] Setting up label_data_1_split
I1122 10:35:18.570689  2348 net.cpp:157] Top shape: 128 (128)
I1122 10:35:18.570693  2348 net.cpp:157] Top shape: 128 (128)
I1122 10:35:18.570695  2348 net.cpp:165] Memory required for data: 77071872
I1122 10:35:18.570699  2348 layer_factory.hpp:77] Creating layer conv1
I1122 10:35:18.570711  2348 net.cpp:100] Creating Layer conv1
I1122 10:35:18.570715  2348 net.cpp:434] conv1 <- data
I1122 10:35:18.570720  2348 net.cpp:408] conv1 -> conv1
I1122 10:35:18.591694  2367 blocking_queue.cpp:50] Waiting for data
I1122 10:35:20.366916  2348 net.cpp:150] Setting up conv1
I1122 10:35:20.366947  2348 net.cpp:157] Top shape: 128 96 109 109 (145993728)
I1122 10:35:20.366952  2348 net.cpp:165] Memory required for data: 661046784
I1122 10:35:20.366966  2348 layer_factory.hpp:77] Creating layer relu1
I1122 10:35:20.366977  2348 net.cpp:100] Creating Layer relu1
I1122 10:35:20.366981  2348 net.cpp:434] relu1 <- conv1
I1122 10:35:20.366987  2348 net.cpp:395] relu1 -> conv1 (in-place)
I1122 10:35:20.367185  2348 net.cpp:150] Setting up relu1
I1122 10:35:20.367203  2348 net.cpp:157] Top shape: 128 96 109 109 (145993728)
I1122 10:35:20.367208  2348 net.cpp:165] Memory required for data: 1245021696
I1122 10:35:20.367213  2348 layer_factory.hpp:77] Creating layer pool1
I1122 10:35:20.367220  2348 net.cpp:100] Creating Layer pool1
I1122 10:35:20.367225  2348 net.cpp:434] pool1 <- conv1
I1122 10:35:20.367230  2348 net.cpp:408] pool1 -> pool1
I1122 10:35:20.367266  2348 net.cpp:150] Setting up pool1
I1122 10:35:20.367272  2348 net.cpp:157] Top shape: 128 96 54 54 (35831808)
I1122 10:35:20.367276  2348 net.cpp:165] Memory required for data: 1388348928
I1122 10:35:20.367280  2348 layer_factory.hpp:77] Creating layer norm1
I1122 10:35:20.367288  2348 net.cpp:100] Creating Layer norm1
I1122 10:35:20.367292  2348 net.cpp:434] norm1 <- pool1
I1122 10:35:20.367297  2348 net.cpp:408] norm1 -> norm1
I1122 10:35:20.367503  2348 net.cpp:150] Setting up norm1
I1122 10:35:20.367513  2348 net.cpp:157] Top shape: 128 96 54 54 (35831808)
I1122 10:35:20.367518  2348 net.cpp:165] Memory required for data: 1531676160
I1122 10:35:20.367522  2348 layer_factory.hpp:77] Creating layer conv2
I1122 10:35:20.367533  2348 net.cpp:100] Creating Layer conv2
I1122 10:35:20.367537  2348 net.cpp:434] conv2 <- norm1
I1122 10:35:20.367543  2348 net.cpp:408] conv2 -> conv2
I1122 10:35:20.375056  2348 net.cpp:150] Setting up conv2
I1122 10:35:20.375069  2348 net.cpp:157] Top shape: 128 256 52 52 (88604672)
I1122 10:35:20.375073  2348 net.cpp:165] Memory required for data: 1886094848
I1122 10:35:20.375083  2348 layer_factory.hpp:77] Creating layer relu2
I1122 10:35:20.375089  2348 net.cpp:100] Creating Layer relu2
I1122 10:35:20.375093  2348 net.cpp:434] relu2 <- conv2
I1122 10:35:20.375098  2348 net.cpp:395] relu2 -> conv2 (in-place)
I1122 10:35:20.375296  2348 net.cpp:150] Setting up relu2
I1122 10:35:20.375308  2348 net.cpp:157] Top shape: 128 256 52 52 (88604672)
I1122 10:35:20.375311  2348 net.cpp:165] Memory required for data: 2240513536
I1122 10:35:20.375316  2348 layer_factory.hpp:77] Creating layer pool2
I1122 10:35:20.375324  2348 net.cpp:100] Creating Layer pool2
I1122 10:35:20.375327  2348 net.cpp:434] pool2 <- conv2
I1122 10:35:20.375332  2348 net.cpp:408] pool2 -> pool2
I1122 10:35:20.375365  2348 net.cpp:150] Setting up pool2
I1122 10:35:20.375372  2348 net.cpp:157] Top shape: 128 256 26 26 (22151168)
I1122 10:35:20.375375  2348 net.cpp:165] Memory required for data: 2329118208
I1122 10:35:20.375380  2348 layer_factory.hpp:77] Creating layer norm2
I1122 10:35:20.375385  2348 net.cpp:100] Creating Layer norm2
I1122 10:35:20.375391  2348 net.cpp:434] norm2 <- pool2
I1122 10:35:20.375396  2348 net.cpp:408] norm2 -> norm2
I1122 10:35:20.375532  2348 net.cpp:150] Setting up norm2
I1122 10:35:20.375540  2348 net.cpp:157] Top shape: 128 256 26 26 (22151168)
I1122 10:35:20.375545  2348 net.cpp:165] Memory required for data: 2417722880
I1122 10:35:20.375547  2348 layer_factory.hpp:77] Creating layer conv3
I1122 10:35:20.375556  2348 net.cpp:100] Creating Layer conv3
I1122 10:35:20.375561  2348 net.cpp:434] conv3 <- norm2
I1122 10:35:20.375567  2348 net.cpp:408] conv3 -> conv3
I1122 10:35:20.399268  2348 net.cpp:150] Setting up conv3
I1122 10:35:20.399286  2348 net.cpp:157] Top shape: 128 512 26 26 (44302336)
I1122 10:35:20.399291  2348 net.cpp:165] Memory required for data: 2594932224
I1122 10:35:20.399299  2348 layer_factory.hpp:77] Creating layer relu3
I1122 10:35:20.399308  2348 net.cpp:100] Creating Layer relu3
I1122 10:35:20.399312  2348 net.cpp:434] relu3 <- conv3
I1122 10:35:20.399317  2348 net.cpp:395] relu3 -> conv3 (in-place)
I1122 10:35:20.399507  2348 net.cpp:150] Setting up relu3
I1122 10:35:20.399518  2348 net.cpp:157] Top shape: 128 512 26 26 (44302336)
I1122 10:35:20.399521  2348 net.cpp:165] Memory required for data: 2772141568
I1122 10:35:20.399524  2348 layer_factory.hpp:77] Creating layer conv4
I1122 10:35:20.399535  2348 net.cpp:100] Creating Layer conv4
I1122 10:35:20.399539  2348 net.cpp:434] conv4 <- conv3
I1122 10:35:20.399550  2348 net.cpp:408] conv4 -> conv4
I1122 10:35:20.422251  2348 net.cpp:150] Setting up conv4
I1122 10:35:20.422269  2348 net.cpp:157] Top shape: 128 512 26 26 (44302336)
I1122 10:35:20.422273  2348 net.cpp:165] Memory required for data: 2949350912
I1122 10:35:20.422279  2348 layer_factory.hpp:77] Creating layer relu4
I1122 10:35:20.422287  2348 net.cpp:100] Creating Layer relu4
I1122 10:35:20.422291  2348 net.cpp:434] relu4 <- conv4
I1122 10:35:20.422297  2348 net.cpp:395] relu4 -> conv4 (in-place)
I1122 10:35:20.422475  2348 net.cpp:150] Setting up relu4
I1122 10:35:20.422485  2348 net.cpp:157] Top shape: 128 512 26 26 (44302336)
I1122 10:35:20.422488  2348 net.cpp:165] Memory required for data: 3126560256
I1122 10:35:20.422492  2348 layer_factory.hpp:77] Creating layer conv5
I1122 10:35:20.422503  2348 net.cpp:100] Creating Layer conv5
I1122 10:35:20.422508  2348 net.cpp:434] conv5 <- conv4
I1122 10:35:20.422513  2348 net.cpp:408] conv5 -> conv5
I1122 10:35:20.444329  2348 net.cpp:150] Setting up conv5
I1122 10:35:20.444347  2348 net.cpp:157] Top shape: 128 512 26 26 (44302336)
I1122 10:35:20.444351  2348 net.cpp:165] Memory required for data: 3303769600
I1122 10:35:20.444362  2348 layer_factory.hpp:77] Creating layer relu5
I1122 10:35:20.444370  2348 net.cpp:100] Creating Layer relu5
I1122 10:35:20.444372  2348 net.cpp:434] relu5 <- conv5
I1122 10:35:20.444377  2348 net.cpp:395] relu5 -> conv5 (in-place)
I1122 10:35:20.444561  2348 net.cpp:150] Setting up relu5
I1122 10:35:20.444571  2348 net.cpp:157] Top shape: 128 512 26 26 (44302336)
I1122 10:35:20.444573  2348 net.cpp:165] Memory required for data: 3480978944
I1122 10:35:20.444577  2348 layer_factory.hpp:77] Creating layer pool5
I1122 10:35:20.444584  2348 net.cpp:100] Creating Layer pool5
I1122 10:35:20.444588  2348 net.cpp:434] pool5 <- conv5
I1122 10:35:20.444593  2348 net.cpp:408] pool5 -> pool5
I1122 10:35:20.444622  2348 net.cpp:150] Setting up pool5
I1122 10:35:20.444628  2348 net.cpp:157] Top shape: 128 512 13 13 (11075584)
I1122 10:35:20.444631  2348 net.cpp:165] Memory required for data: 3525281280
I1122 10:35:20.444634  2348 layer_factory.hpp:77] Creating layer fc6
I1122 10:35:20.444641  2348 net.cpp:100] Creating Layer fc6
I1122 10:35:20.444645  2348 net.cpp:434] fc6 <- pool5
I1122 10:35:20.444650  2348 net.cpp:408] fc6 -> fc6
I1122 10:35:26.525017  2348 net.cpp:150] Setting up fc6
I1122 10:35:26.525049  2348 net.cpp:157] Top shape: 128 4096 (524288)
I1122 10:35:26.525054  2348 net.cpp:165] Memory required for data: 3527378432
I1122 10:35:26.525063  2348 layer_factory.hpp:77] Creating layer relu6
I1122 10:35:26.525071  2348 net.cpp:100] Creating Layer relu6
I1122 10:35:26.525075  2348 net.cpp:434] relu6 <- fc6
I1122 10:35:26.525081  2348 net.cpp:395] relu6 -> fc6 (in-place)
I1122 10:35:26.525238  2348 net.cpp:150] Setting up relu6
I1122 10:35:26.525248  2348 net.cpp:157] Top shape: 128 4096 (524288)
I1122 10:35:26.525250  2348 net.cpp:165] Memory required for data: 3529475584
I1122 10:35:26.525254  2348 layer_factory.hpp:77] Creating layer drop6
I1122 10:35:26.525260  2348 net.cpp:100] Creating Layer drop6
I1122 10:35:26.525264  2348 net.cpp:434] drop6 <- fc6
I1122 10:35:26.525270  2348 net.cpp:395] drop6 -> fc6 (in-place)
I1122 10:35:26.525290  2348 net.cpp:150] Setting up drop6
I1122 10:35:26.525295  2348 net.cpp:157] Top shape: 128 4096 (524288)
I1122 10:35:26.525298  2348 net.cpp:165] Memory required for data: 3531572736
I1122 10:35:26.525301  2348 layer_factory.hpp:77] Creating layer fc7
I1122 10:35:26.525310  2348 net.cpp:100] Creating Layer fc7
I1122 10:35:26.525313  2348 net.cpp:434] fc7 <- fc6
I1122 10:35:26.525318  2348 net.cpp:408] fc7 -> fc7
I1122 10:35:26.669410  2348 net.cpp:150] Setting up fc7
I1122 10:35:26.669441  2348 net.cpp:157] Top shape: 128 2048 (262144)
I1122 10:35:26.669445  2348 net.cpp:165] Memory required for data: 3532621312
I1122 10:35:26.669455  2348 layer_factory.hpp:77] Creating layer relu7
I1122 10:35:26.669463  2348 net.cpp:100] Creating Layer relu7
I1122 10:35:26.669467  2348 net.cpp:434] relu7 <- fc7
I1122 10:35:26.669479  2348 net.cpp:395] relu7 -> fc7 (in-place)
I1122 10:35:26.669778  2348 net.cpp:150] Setting up relu7
I1122 10:35:26.669790  2348 net.cpp:157] Top shape: 128 2048 (262144)
I1122 10:35:26.669795  2348 net.cpp:165] Memory required for data: 3533669888
I1122 10:35:26.669798  2348 layer_factory.hpp:77] Creating layer drop7
I1122 10:35:26.669805  2348 net.cpp:100] Creating Layer drop7
I1122 10:35:26.669808  2348 net.cpp:434] drop7 <- fc7
I1122 10:35:26.669813  2348 net.cpp:395] drop7 -> fc7 (in-place)
I1122 10:35:26.669832  2348 net.cpp:150] Setting up drop7
I1122 10:35:26.669837  2348 net.cpp:157] Top shape: 128 2048 (262144)
I1122 10:35:26.669841  2348 net.cpp:165] Memory required for data: 3534718464
I1122 10:35:26.669843  2348 layer_factory.hpp:77] Creating layer fc8
I1122 10:35:26.669852  2348 net.cpp:100] Creating Layer fc8
I1122 10:35:26.669855  2348 net.cpp:434] fc8 <- fc7
I1122 10:35:26.669862  2348 net.cpp:408] fc8 -> fc8
I1122 10:35:26.673624  2348 net.cpp:150] Setting up fc8
I1122 10:35:26.673635  2348 net.cpp:157] Top shape: 128 101 (12928)
I1122 10:35:26.673640  2348 net.cpp:165] Memory required for data: 3534770176
I1122 10:35:26.673645  2348 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1122 10:35:26.673652  2348 net.cpp:100] Creating Layer fc8_fc8_0_split
I1122 10:35:26.673655  2348 net.cpp:434] fc8_fc8_0_split <- fc8
I1122 10:35:26.673660  2348 net.cpp:408] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1122 10:35:26.673666  2348 net.cpp:408] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1122 10:35:26.673688  2348 net.cpp:150] Setting up fc8_fc8_0_split
I1122 10:35:26.673694  2348 net.cpp:157] Top shape: 128 101 (12928)
I1122 10:35:26.673697  2348 net.cpp:157] Top shape: 128 101 (12928)
I1122 10:35:26.673701  2348 net.cpp:165] Memory required for data: 3534873600
I1122 10:35:26.673703  2348 layer_factory.hpp:77] Creating layer accuracy
I1122 10:35:26.673709  2348 net.cpp:100] Creating Layer accuracy
I1122 10:35:26.673713  2348 net.cpp:434] accuracy <- fc8_fc8_0_split_0
I1122 10:35:26.673717  2348 net.cpp:434] accuracy <- label_data_1_split_0
I1122 10:35:26.673722  2348 net.cpp:408] accuracy -> accuracy
I1122 10:35:26.673728  2348 net.cpp:150] Setting up accuracy
I1122 10:35:26.673732  2348 net.cpp:157] Top shape: (1)
I1122 10:35:26.673735  2348 net.cpp:165] Memory required for data: 3534873604
I1122 10:35:26.673738  2348 layer_factory.hpp:77] Creating layer loss
I1122 10:35:26.673743  2348 net.cpp:100] Creating Layer loss
I1122 10:35:26.673746  2348 net.cpp:434] loss <- fc8_fc8_0_split_1
I1122 10:35:26.673750  2348 net.cpp:434] loss <- label_data_1_split_1
I1122 10:35:26.673755  2348 net.cpp:408] loss -> loss
I1122 10:35:26.673763  2348 layer_factory.hpp:77] Creating layer loss
I1122 10:35:26.673954  2348 net.cpp:150] Setting up loss
I1122 10:35:26.673961  2348 net.cpp:157] Top shape: (1)
I1122 10:35:26.673964  2348 net.cpp:160]     with loss weight 1
I1122 10:35:26.673979  2348 net.cpp:165] Memory required for data: 3534873608
I1122 10:35:26.673982  2348 net.cpp:226] loss needs backward computation.
I1122 10:35:26.673985  2348 net.cpp:228] accuracy does not need backward computation.
I1122 10:35:26.673990  2348 net.cpp:226] fc8_fc8_0_split needs backward computation.
I1122 10:35:26.673992  2348 net.cpp:226] fc8 needs backward computation.
I1122 10:35:26.673995  2348 net.cpp:226] drop7 needs backward computation.
I1122 10:35:26.673998  2348 net.cpp:226] relu7 needs backward computation.
I1122 10:35:26.674001  2348 net.cpp:226] fc7 needs backward computation.
I1122 10:35:26.674005  2348 net.cpp:226] drop6 needs backward computation.
I1122 10:35:26.674008  2348 net.cpp:226] relu6 needs backward computation.
I1122 10:35:26.674011  2348 net.cpp:226] fc6 needs backward computation.
I1122 10:35:26.674015  2348 net.cpp:226] pool5 needs backward computation.
I1122 10:35:26.674017  2348 net.cpp:226] relu5 needs backward computation.
I1122 10:35:26.674021  2348 net.cpp:226] conv5 needs backward computation.
I1122 10:35:26.674024  2348 net.cpp:226] relu4 needs backward computation.
I1122 10:35:26.674027  2348 net.cpp:226] conv4 needs backward computation.
I1122 10:35:26.674038  2348 net.cpp:226] relu3 needs backward computation.
I1122 10:35:26.674041  2348 net.cpp:226] conv3 needs backward computation.
I1122 10:35:26.674046  2348 net.cpp:226] norm2 needs backward computation.
I1122 10:35:26.674048  2348 net.cpp:226] pool2 needs backward computation.
I1122 10:35:26.674052  2348 net.cpp:226] relu2 needs backward computation.
I1122 10:35:26.674055  2348 net.cpp:226] conv2 needs backward computation.
I1122 10:35:26.674058  2348 net.cpp:226] norm1 needs backward computation.
I1122 10:35:26.674062  2348 net.cpp:226] pool1 needs backward computation.
I1122 10:35:26.674064  2348 net.cpp:226] relu1 needs backward computation.
I1122 10:35:26.674067  2348 net.cpp:226] conv1 needs backward computation.
I1122 10:35:26.674072  2348 net.cpp:228] label_data_1_split does not need backward computation.
I1122 10:35:26.674075  2348 net.cpp:228] data does not need backward computation.
I1122 10:35:26.674078  2348 net.cpp:270] This network produces output accuracy
I1122 10:35:26.674082  2348 net.cpp:270] This network produces output loss
I1122 10:35:26.674095  2348 net.cpp:283] Network initialization done.
I1122 10:35:26.674567  2348 solver.cpp:181] Creating test net (#0) specified by net file: mywork/ucf101/vgg_m_2048_net.prototxt
I1122 10:35:26.674602  2348 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1122 10:35:26.674778  2348 net.cpp:58] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 224
    mean_file: "mywork/ucf101/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/home/hadoop/whx/dataset/ucf101/jpgoutput/split1/ucf101_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 101
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1122 10:35:26.674860  2348 layer_factory.hpp:77] Creating layer data
I1122 10:35:26.674916  2348 net.cpp:100] Creating Layer data
I1122 10:35:26.674922  2348 net.cpp:408] data -> data
I1122 10:35:26.674932  2348 net.cpp:408] data -> label
I1122 10:35:26.674938  2348 data_transformer.cpp:25] Loading mean file from: mywork/ucf101/imagenet_mean.binaryproto
I1122 10:35:26.727941  2368 db_lmdb.cpp:35] Opened lmdb /home/hadoop/whx/dataset/ucf101/jpgoutput/split1/ucf101_val_lmdb
I1122 10:35:26.751519  2348 data_layer.cpp:41] output data size: 50,3,224,224
I1122 10:35:26.781275  2348 net.cpp:150] Setting up data
I1122 10:35:26.781303  2348 net.cpp:157] Top shape: 50 3 224 224 (7526400)
I1122 10:35:26.781309  2348 net.cpp:157] Top shape: 50 (50)
I1122 10:35:26.781312  2348 net.cpp:165] Memory required for data: 30105800
I1122 10:35:26.781318  2348 layer_factory.hpp:77] Creating layer label_data_1_split
I1122 10:35:26.781328  2348 net.cpp:100] Creating Layer label_data_1_split
I1122 10:35:26.781333  2348 net.cpp:434] label_data_1_split <- label
I1122 10:35:26.781339  2348 net.cpp:408] label_data_1_split -> label_data_1_split_0
I1122 10:35:26.781347  2348 net.cpp:408] label_data_1_split -> label_data_1_split_1
I1122 10:35:26.781390  2348 net.cpp:150] Setting up label_data_1_split
I1122 10:35:26.781396  2348 net.cpp:157] Top shape: 50 (50)
I1122 10:35:26.781400  2348 net.cpp:157] Top shape: 50 (50)
I1122 10:35:26.781404  2348 net.cpp:165] Memory required for data: 30106200
I1122 10:35:26.781406  2348 layer_factory.hpp:77] Creating layer conv1
I1122 10:35:26.781417  2348 net.cpp:100] Creating Layer conv1
I1122 10:35:26.781425  2348 net.cpp:434] conv1 <- data
I1122 10:35:26.781436  2348 net.cpp:408] conv1 -> conv1
I1122 10:35:26.785326  2348 net.cpp:150] Setting up conv1
I1122 10:35:26.785341  2348 net.cpp:157] Top shape: 50 96 109 109 (57028800)
I1122 10:35:26.785344  2348 net.cpp:165] Memory required for data: 258221400
I1122 10:35:26.785353  2348 layer_factory.hpp:77] Creating layer relu1
I1122 10:35:26.785361  2348 net.cpp:100] Creating Layer relu1
I1122 10:35:26.785363  2348 net.cpp:434] relu1 <- conv1
I1122 10:35:26.785368  2348 net.cpp:395] relu1 -> conv1 (in-place)
I1122 10:35:26.785559  2348 net.cpp:150] Setting up relu1
I1122 10:35:26.785579  2348 net.cpp:157] Top shape: 50 96 109 109 (57028800)
I1122 10:35:26.785584  2348 net.cpp:165] Memory required for data: 486336600
I1122 10:35:26.785588  2348 layer_factory.hpp:77] Creating layer pool1
I1122 10:35:26.785596  2348 net.cpp:100] Creating Layer pool1
I1122 10:35:26.785599  2348 net.cpp:434] pool1 <- conv1
I1122 10:35:26.785604  2348 net.cpp:408] pool1 -> pool1
I1122 10:35:26.785635  2348 net.cpp:150] Setting up pool1
I1122 10:35:26.785641  2348 net.cpp:157] Top shape: 50 96 54 54 (13996800)
I1122 10:35:26.785645  2348 net.cpp:165] Memory required for data: 542323800
I1122 10:35:26.785647  2348 layer_factory.hpp:77] Creating layer norm1
I1122 10:35:26.785653  2348 net.cpp:100] Creating Layer norm1
I1122 10:35:26.785657  2348 net.cpp:434] norm1 <- pool1
I1122 10:35:26.785661  2348 net.cpp:408] norm1 -> norm1
I1122 10:35:26.785784  2348 net.cpp:150] Setting up norm1
I1122 10:35:26.785790  2348 net.cpp:157] Top shape: 50 96 54 54 (13996800)
I1122 10:35:26.785794  2348 net.cpp:165] Memory required for data: 598311000
I1122 10:35:26.785797  2348 layer_factory.hpp:77] Creating layer conv2
I1122 10:35:26.785805  2348 net.cpp:100] Creating Layer conv2
I1122 10:35:26.785809  2348 net.cpp:434] conv2 <- norm1
I1122 10:35:26.785815  2348 net.cpp:408] conv2 -> conv2
I1122 10:35:26.792403  2348 net.cpp:150] Setting up conv2
I1122 10:35:26.792425  2348 net.cpp:157] Top shape: 50 256 52 52 (34611200)
I1122 10:35:26.792429  2348 net.cpp:165] Memory required for data: 736755800
I1122 10:35:26.792439  2348 layer_factory.hpp:77] Creating layer relu2
I1122 10:35:26.792448  2348 net.cpp:100] Creating Layer relu2
I1122 10:35:26.792451  2348 net.cpp:434] relu2 <- conv2
I1122 10:35:26.792456  2348 net.cpp:395] relu2 -> conv2 (in-place)
I1122 10:35:26.792565  2348 net.cpp:150] Setting up relu2
I1122 10:35:26.792572  2348 net.cpp:157] Top shape: 50 256 52 52 (34611200)
I1122 10:35:26.792575  2348 net.cpp:165] Memory required for data: 875200600
I1122 10:35:26.792578  2348 layer_factory.hpp:77] Creating layer pool2
I1122 10:35:26.792587  2348 net.cpp:100] Creating Layer pool2
I1122 10:35:26.792589  2348 net.cpp:434] pool2 <- conv2
I1122 10:35:26.792594  2348 net.cpp:408] pool2 -> pool2
I1122 10:35:26.792624  2348 net.cpp:150] Setting up pool2
I1122 10:35:26.792630  2348 net.cpp:157] Top shape: 50 256 26 26 (8652800)
I1122 10:35:26.792634  2348 net.cpp:165] Memory required for data: 909811800
I1122 10:35:26.792636  2348 layer_factory.hpp:77] Creating layer norm2
I1122 10:35:26.792642  2348 net.cpp:100] Creating Layer norm2
I1122 10:35:26.792645  2348 net.cpp:434] norm2 <- pool2
I1122 10:35:26.792650  2348 net.cpp:408] norm2 -> norm2
I1122 10:35:26.792840  2348 net.cpp:150] Setting up norm2
I1122 10:35:26.792848  2348 net.cpp:157] Top shape: 50 256 26 26 (8652800)
I1122 10:35:26.792851  2348 net.cpp:165] Memory required for data: 944423000
I1122 10:35:26.792855  2348 layer_factory.hpp:77] Creating layer conv3
I1122 10:35:26.792863  2348 net.cpp:100] Creating Layer conv3
I1122 10:35:26.792867  2348 net.cpp:434] conv3 <- norm2
I1122 10:35:26.792872  2348 net.cpp:408] conv3 -> conv3
I1122 10:35:26.814092  2348 net.cpp:150] Setting up conv3
I1122 10:35:26.814116  2348 net.cpp:157] Top shape: 50 512 26 26 (17305600)
I1122 10:35:26.814119  2348 net.cpp:165] Memory required for data: 1013645400
I1122 10:35:26.814129  2348 layer_factory.hpp:77] Creating layer relu3
I1122 10:35:26.814138  2348 net.cpp:100] Creating Layer relu3
I1122 10:35:26.814147  2348 net.cpp:434] relu3 <- conv3
I1122 10:35:26.814158  2348 net.cpp:395] relu3 -> conv3 (in-place)
I1122 10:35:26.814268  2348 net.cpp:150] Setting up relu3
I1122 10:35:26.814275  2348 net.cpp:157] Top shape: 50 512 26 26 (17305600)
I1122 10:35:26.814278  2348 net.cpp:165] Memory required for data: 1082867800
I1122 10:35:26.814282  2348 layer_factory.hpp:77] Creating layer conv4
I1122 10:35:26.814291  2348 net.cpp:100] Creating Layer conv4
I1122 10:35:26.814296  2348 net.cpp:434] conv4 <- conv3
I1122 10:35:26.814301  2348 net.cpp:408] conv4 -> conv4
I1122 10:35:26.835927  2348 net.cpp:150] Setting up conv4
I1122 10:35:26.835950  2348 net.cpp:157] Top shape: 50 512 26 26 (17305600)
I1122 10:35:26.835955  2348 net.cpp:165] Memory required for data: 1152090200
I1122 10:35:26.835963  2348 layer_factory.hpp:77] Creating layer relu4
I1122 10:35:26.835970  2348 net.cpp:100] Creating Layer relu4
I1122 10:35:26.835974  2348 net.cpp:434] relu4 <- conv4
I1122 10:35:26.835979  2348 net.cpp:395] relu4 -> conv4 (in-place)
I1122 10:35:26.836092  2348 net.cpp:150] Setting up relu4
I1122 10:35:26.836099  2348 net.cpp:157] Top shape: 50 512 26 26 (17305600)
I1122 10:35:26.836103  2348 net.cpp:165] Memory required for data: 1221312600
I1122 10:35:26.836107  2348 layer_factory.hpp:77] Creating layer conv5
I1122 10:35:26.836115  2348 net.cpp:100] Creating Layer conv5
I1122 10:35:26.836119  2348 net.cpp:434] conv5 <- conv4
I1122 10:35:26.836124  2348 net.cpp:408] conv5 -> conv5
I1122 10:35:26.857426  2348 net.cpp:150] Setting up conv5
I1122 10:35:26.857450  2348 net.cpp:157] Top shape: 50 512 26 26 (17305600)
I1122 10:35:26.857455  2348 net.cpp:165] Memory required for data: 1290535000
I1122 10:35:26.857465  2348 layer_factory.hpp:77] Creating layer relu5
I1122 10:35:26.857475  2348 net.cpp:100] Creating Layer relu5
I1122 10:35:26.857478  2348 net.cpp:434] relu5 <- conv5
I1122 10:35:26.857483  2348 net.cpp:395] relu5 -> conv5 (in-place)
I1122 10:35:26.857661  2348 net.cpp:150] Setting up relu5
I1122 10:35:26.857671  2348 net.cpp:157] Top shape: 50 512 26 26 (17305600)
I1122 10:35:26.857674  2348 net.cpp:165] Memory required for data: 1359757400
I1122 10:35:26.857677  2348 layer_factory.hpp:77] Creating layer pool5
I1122 10:35:26.857686  2348 net.cpp:100] Creating Layer pool5
I1122 10:35:26.857688  2348 net.cpp:434] pool5 <- conv5
I1122 10:35:26.857693  2348 net.cpp:408] pool5 -> pool5
I1122 10:35:26.857723  2348 net.cpp:150] Setting up pool5
I1122 10:35:26.857729  2348 net.cpp:157] Top shape: 50 512 13 13 (4326400)
I1122 10:35:26.857733  2348 net.cpp:165] Memory required for data: 1377063000
I1122 10:35:26.857735  2348 layer_factory.hpp:77] Creating layer fc6
I1122 10:35:26.857741  2348 net.cpp:100] Creating Layer fc6
I1122 10:35:26.857745  2348 net.cpp:434] fc6 <- pool5
I1122 10:35:26.857749  2348 net.cpp:408] fc6 -> fc6
I1122 10:35:32.945775  2348 net.cpp:150] Setting up fc6
I1122 10:35:32.945808  2348 net.cpp:157] Top shape: 50 4096 (204800)
I1122 10:35:32.945812  2348 net.cpp:165] Memory required for data: 1377882200
I1122 10:35:32.945822  2348 layer_factory.hpp:77] Creating layer relu6
I1122 10:35:32.945830  2348 net.cpp:100] Creating Layer relu6
I1122 10:35:32.945835  2348 net.cpp:434] relu6 <- fc6
I1122 10:35:32.945842  2348 net.cpp:395] relu6 -> fc6 (in-place)
I1122 10:35:32.945999  2348 net.cpp:150] Setting up relu6
I1122 10:35:32.946008  2348 net.cpp:157] Top shape: 50 4096 (204800)
I1122 10:35:32.946012  2348 net.cpp:165] Memory required for data: 1378701400
I1122 10:35:32.946015  2348 layer_factory.hpp:77] Creating layer drop6
I1122 10:35:32.946022  2348 net.cpp:100] Creating Layer drop6
I1122 10:35:32.946025  2348 net.cpp:434] drop6 <- fc6
I1122 10:35:32.946029  2348 net.cpp:395] drop6 -> fc6 (in-place)
I1122 10:35:32.946053  2348 net.cpp:150] Setting up drop6
I1122 10:35:32.946058  2348 net.cpp:157] Top shape: 50 4096 (204800)
I1122 10:35:32.946061  2348 net.cpp:165] Memory required for data: 1379520600
I1122 10:35:32.946064  2348 layer_factory.hpp:77] Creating layer fc7
I1122 10:35:32.946079  2348 net.cpp:100] Creating Layer fc7
I1122 10:35:32.946087  2348 net.cpp:434] fc7 <- fc6
I1122 10:35:32.946094  2348 net.cpp:408] fc7 -> fc7
I1122 10:35:33.090330  2348 net.cpp:150] Setting up fc7
I1122 10:35:33.090360  2348 net.cpp:157] Top shape: 50 2048 (102400)
I1122 10:35:33.090364  2348 net.cpp:165] Memory required for data: 1379930200
I1122 10:35:33.090373  2348 layer_factory.hpp:77] Creating layer relu7
I1122 10:35:33.090381  2348 net.cpp:100] Creating Layer relu7
I1122 10:35:33.090385  2348 net.cpp:434] relu7 <- fc7
I1122 10:35:33.090391  2348 net.cpp:395] relu7 -> fc7 (in-place)
I1122 10:35:33.090668  2348 net.cpp:150] Setting up relu7
I1122 10:35:33.090679  2348 net.cpp:157] Top shape: 50 2048 (102400)
I1122 10:35:33.090683  2348 net.cpp:165] Memory required for data: 1380339800
I1122 10:35:33.090687  2348 layer_factory.hpp:77] Creating layer drop7
I1122 10:35:33.090693  2348 net.cpp:100] Creating Layer drop7
I1122 10:35:33.090697  2348 net.cpp:434] drop7 <- fc7
I1122 10:35:33.090701  2348 net.cpp:395] drop7 -> fc7 (in-place)
I1122 10:35:33.090726  2348 net.cpp:150] Setting up drop7
I1122 10:35:33.090731  2348 net.cpp:157] Top shape: 50 2048 (102400)
I1122 10:35:33.090734  2348 net.cpp:165] Memory required for data: 1380749400
I1122 10:35:33.090737  2348 layer_factory.hpp:77] Creating layer fc8
I1122 10:35:33.090744  2348 net.cpp:100] Creating Layer fc8
I1122 10:35:33.090747  2348 net.cpp:434] fc8 <- fc7
I1122 10:35:33.090752  2348 net.cpp:408] fc8 -> fc8
I1122 10:35:33.094535  2348 net.cpp:150] Setting up fc8
I1122 10:35:33.094547  2348 net.cpp:157] Top shape: 50 101 (5050)
I1122 10:35:33.094550  2348 net.cpp:165] Memory required for data: 1380769600
I1122 10:35:33.094557  2348 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1122 10:35:33.094563  2348 net.cpp:100] Creating Layer fc8_fc8_0_split
I1122 10:35:33.094565  2348 net.cpp:434] fc8_fc8_0_split <- fc8
I1122 10:35:33.094570  2348 net.cpp:408] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1122 10:35:33.094576  2348 net.cpp:408] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1122 10:35:33.094604  2348 net.cpp:150] Setting up fc8_fc8_0_split
I1122 10:35:33.094609  2348 net.cpp:157] Top shape: 50 101 (5050)
I1122 10:35:33.094611  2348 net.cpp:157] Top shape: 50 101 (5050)
I1122 10:35:33.094615  2348 net.cpp:165] Memory required for data: 1380810000
I1122 10:35:33.094619  2348 layer_factory.hpp:77] Creating layer accuracy
I1122 10:35:33.094622  2348 net.cpp:100] Creating Layer accuracy
I1122 10:35:33.094626  2348 net.cpp:434] accuracy <- fc8_fc8_0_split_0
I1122 10:35:33.094630  2348 net.cpp:434] accuracy <- label_data_1_split_0
I1122 10:35:33.094635  2348 net.cpp:408] accuracy -> accuracy
I1122 10:35:33.094640  2348 net.cpp:150] Setting up accuracy
I1122 10:35:33.094645  2348 net.cpp:157] Top shape: (1)
I1122 10:35:33.094647  2348 net.cpp:165] Memory required for data: 1380810004
I1122 10:35:33.094650  2348 layer_factory.hpp:77] Creating layer loss
I1122 10:35:33.094655  2348 net.cpp:100] Creating Layer loss
I1122 10:35:33.094658  2348 net.cpp:434] loss <- fc8_fc8_0_split_1
I1122 10:35:33.094662  2348 net.cpp:434] loss <- label_data_1_split_1
I1122 10:35:33.094666  2348 net.cpp:408] loss -> loss
I1122 10:35:33.094673  2348 layer_factory.hpp:77] Creating layer loss
I1122 10:35:33.094858  2348 net.cpp:150] Setting up loss
I1122 10:35:33.094866  2348 net.cpp:157] Top shape: (1)
I1122 10:35:33.094869  2348 net.cpp:160]     with loss weight 1
I1122 10:35:33.094877  2348 net.cpp:165] Memory required for data: 1380810008
I1122 10:35:33.094880  2348 net.cpp:226] loss needs backward computation.
I1122 10:35:33.094884  2348 net.cpp:228] accuracy does not need backward computation.
I1122 10:35:33.094888  2348 net.cpp:226] fc8_fc8_0_split needs backward computation.
I1122 10:35:33.094892  2348 net.cpp:226] fc8 needs backward computation.
I1122 10:35:33.094894  2348 net.cpp:226] drop7 needs backward computation.
I1122 10:35:33.094897  2348 net.cpp:226] relu7 needs backward computation.
I1122 10:35:33.094900  2348 net.cpp:226] fc7 needs backward computation.
I1122 10:35:33.094909  2348 net.cpp:226] drop6 needs backward computation.
I1122 10:35:33.094918  2348 net.cpp:226] relu6 needs backward computation.
I1122 10:35:33.094920  2348 net.cpp:226] fc6 needs backward computation.
I1122 10:35:33.094923  2348 net.cpp:226] pool5 needs backward computation.
I1122 10:35:33.094928  2348 net.cpp:226] relu5 needs backward computation.
I1122 10:35:33.094930  2348 net.cpp:226] conv5 needs backward computation.
I1122 10:35:33.094934  2348 net.cpp:226] relu4 needs backward computation.
I1122 10:35:33.094938  2348 net.cpp:226] conv4 needs backward computation.
I1122 10:35:33.094940  2348 net.cpp:226] relu3 needs backward computation.
I1122 10:35:33.094944  2348 net.cpp:226] conv3 needs backward computation.
I1122 10:35:33.094947  2348 net.cpp:226] norm2 needs backward computation.
I1122 10:35:33.094950  2348 net.cpp:226] pool2 needs backward computation.
I1122 10:35:33.094954  2348 net.cpp:226] relu2 needs backward computation.
I1122 10:35:33.094957  2348 net.cpp:226] conv2 needs backward computation.
I1122 10:35:33.094960  2348 net.cpp:226] norm1 needs backward computation.
I1122 10:35:33.094964  2348 net.cpp:226] pool1 needs backward computation.
I1122 10:35:33.094967  2348 net.cpp:226] relu1 needs backward computation.
I1122 10:35:33.094970  2348 net.cpp:226] conv1 needs backward computation.
I1122 10:35:33.094974  2348 net.cpp:228] label_data_1_split does not need backward computation.
I1122 10:35:33.094977  2348 net.cpp:228] data does not need backward computation.
I1122 10:35:33.094980  2348 net.cpp:270] This network produces output accuracy
I1122 10:35:33.094985  2348 net.cpp:270] This network produces output loss
I1122 10:35:33.094995  2348 net.cpp:283] Network initialization done.
I1122 10:35:33.095077  2348 solver.cpp:60] Solver scaffolding done.
I1122 10:35:33.095438  2348 caffe.cpp:251] Starting Optimization
I1122 10:35:33.095443  2348 solver.cpp:279] Solving CaffeNet
I1122 10:35:33.095446  2348 solver.cpp:280] Learning Rate Policy: step
I1122 10:35:33.096648  2348 solver.cpp:337] Iteration 0, Testing net (#0)
I1122 10:36:46.014969  2348 solver.cpp:404]     Test net output #0: accuracy = 0.0054
I1122 10:36:46.015031  2348 solver.cpp:404]     Test net output #1: loss = 4.72614 (* 1 = 4.72614 loss)
I1122 10:36:46.596446  2348 solver.cpp:228] Iteration 0, loss = 4.93108
I1122 10:36:46.596472  2348 solver.cpp:244]     Train net output #0: accuracy = 0.015625
I1122 10:36:46.596482  2348 solver.cpp:244]     Train net output #1: loss = 4.93108 (* 1 = 4.93108 loss)
I1122 10:36:46.596489  2348 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1122 10:54:56.928329  2348 solver.cpp:337] Iteration 500, Testing net (#0)
I1122 10:56:11.040068  2348 solver.cpp:404]     Test net output #0: accuracy = 0.149667
I1122 10:56:11.040148  2348 solver.cpp:404]     Test net output #1: loss = 3.58268 (* 1 = 3.58268 loss)
I1122 10:56:11.602980  2348 solver.cpp:228] Iteration 500, loss = 3.02991
I1122 10:56:11.603009  2348 solver.cpp:244]     Train net output #0: accuracy = 0.257812
I1122 10:56:11.603019  2348 solver.cpp:244]     Train net output #1: loss = 3.02991 (* 1 = 3.02991 loss)
I1122 10:56:11.603026  2348 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I1122 11:14:21.128082  2348 solver.cpp:337] Iteration 1000, Testing net (#0)
I1122 11:15:35.163913  2348 solver.cpp:404]     Test net output #0: accuracy = 0.247333
I1122 11:15:35.163967  2348 solver.cpp:404]     Test net output #1: loss = 3.56512 (* 1 = 3.56512 loss)
I1122 11:15:35.725260  2348 solver.cpp:228] Iteration 1000, loss = 1.47491
I1122 11:15:35.725294  2348 solver.cpp:244]     Train net output #0: accuracy = 0.609375
I1122 11:15:35.725303  2348 solver.cpp:244]     Train net output #1: loss = 1.47491 (* 1 = 1.47491 loss)
I1122 11:15:35.725311  2348 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I1122 11:33:45.180541  2348 solver.cpp:337] Iteration 1500, Testing net (#0)
I1122 11:34:59.237210  2348 solver.cpp:404]     Test net output #0: accuracy = 0.278467
I1122 11:34:59.237294  2348 solver.cpp:404]     Test net output #1: loss = 3.52796 (* 1 = 3.52796 loss)
I1122 11:34:59.798612  2348 solver.cpp:228] Iteration 1500, loss = 0.936102
I1122 11:34:59.798647  2348 solver.cpp:244]     Train net output #0: accuracy = 0.742188
I1122 11:34:59.798658  2348 solver.cpp:244]     Train net output #1: loss = 0.936102 (* 1 = 0.936102 loss)
I1122 11:34:59.798666  2348 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I1122 11:53:09.332636  2348 solver.cpp:337] Iteration 2000, Testing net (#0)
I1122 11:54:23.408982  2348 solver.cpp:404]     Test net output #0: accuracy = 0.296667
I1122 11:54:23.426923  2348 solver.cpp:404]     Test net output #1: loss = 3.77548 (* 1 = 3.77548 loss)
I1122 11:54:23.988028  2348 solver.cpp:228] Iteration 2000, loss = 0.676338
I1122 11:54:23.988060  2348 solver.cpp:244]     Train net output #0: accuracy = 0.78125
I1122 11:54:23.988070  2348 solver.cpp:244]     Train net output #1: loss = 0.676338 (* 1 = 0.676338 loss)
I1122 11:54:23.988075  2348 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I1122 12:12:33.569085  2348 solver.cpp:337] Iteration 2500, Testing net (#0)
I1122 12:13:47.542614  2348 solver.cpp:404]     Test net output #0: accuracy = 0.316334
I1122 12:13:47.542699  2348 solver.cpp:404]     Test net output #1: loss = 3.56758 (* 1 = 3.56758 loss)
I1122 12:13:48.106984  2348 solver.cpp:228] Iteration 2500, loss = 0.54556
I1122 12:13:48.107017  2348 solver.cpp:244]     Train net output #0: accuracy = 0.828125
I1122 12:13:48.107024  2348 solver.cpp:244]     Train net output #1: loss = 0.54556 (* 1 = 0.54556 loss)
I1122 12:13:48.107030  2348 sgd_solver.cpp:106] Iteration 2500, lr = 0.01
I1122 12:31:57.446732  2348 solver.cpp:337] Iteration 3000, Testing net (#0)
I1122 12:33:11.413743  2348 solver.cpp:404]     Test net output #0: accuracy = 0.327533
I1122 12:33:11.413815  2348 solver.cpp:404]     Test net output #1: loss = 3.6081 (* 1 = 3.6081 loss)
I1122 12:33:11.973414  2348 solver.cpp:228] Iteration 3000, loss = 0.395535
I1122 12:33:11.973448  2348 solver.cpp:244]     Train net output #0: accuracy = 0.875
I1122 12:33:11.973455  2348 solver.cpp:244]     Train net output #1: loss = 0.395535 (* 1 = 0.395535 loss)
I1122 12:33:11.973461  2348 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I1122 12:51:21.331571  2348 solver.cpp:337] Iteration 3500, Testing net (#0)
I1122 12:52:35.340095  2348 solver.cpp:404]     Test net output #0: accuracy = 0.337
I1122 12:52:35.340157  2348 solver.cpp:404]     Test net output #1: loss = 3.69135 (* 1 = 3.69135 loss)
I1122 12:52:35.900871  2348 solver.cpp:228] Iteration 3500, loss = 0.791129
I1122 12:52:35.900904  2348 solver.cpp:244]     Train net output #0: accuracy = 0.835938
I1122 12:52:35.900913  2348 solver.cpp:244]     Train net output #1: loss = 0.791129 (* 1 = 0.791129 loss)
I1122 12:52:35.900918  2348 sgd_solver.cpp:106] Iteration 3500, lr = 0.01
I1122 13:10:45.287688  2348 solver.cpp:337] Iteration 4000, Testing net (#0)
I1122 13:11:59.394701  2348 solver.cpp:404]     Test net output #0: accuracy = 0.3114
I1122 13:11:59.394754  2348 solver.cpp:404]     Test net output #1: loss = 3.88237 (* 1 = 3.88237 loss)
I1122 13:11:59.955512  2348 solver.cpp:228] Iteration 4000, loss = 0.352321
I1122 13:11:59.955538  2348 solver.cpp:244]     Train net output #0: accuracy = 0.898438
I1122 13:11:59.955546  2348 solver.cpp:244]     Train net output #1: loss = 0.352321 (* 1 = 0.352321 loss)
I1122 13:11:59.955552  2348 sgd_solver.cpp:106] Iteration 4000, lr = 0.01
I1122 13:30:09.426658  2348 solver.cpp:337] Iteration 4500, Testing net (#0)
I1122 13:31:23.511776  2348 solver.cpp:404]     Test net output #0: accuracy = 0.343733
I1122 13:31:23.511848  2348 solver.cpp:404]     Test net output #1: loss = 3.71788 (* 1 = 3.71788 loss)
I1122 13:31:24.072405  2348 solver.cpp:228] Iteration 4500, loss = 0.235228
I1122 13:31:24.072439  2348 solver.cpp:244]     Train net output #0: accuracy = 0.9375
I1122 13:31:24.072449  2348 solver.cpp:244]     Train net output #1: loss = 0.235228 (* 1 = 0.235228 loss)
I1122 13:31:24.072456  2348 sgd_solver.cpp:106] Iteration 4500, lr = 0.01
I1122 13:49:33.634768  2348 solver.cpp:337] Iteration 5000, Testing net (#0)
I1122 13:50:47.688745  2348 solver.cpp:404]     Test net output #0: accuracy = 0.345733
I1122 13:50:47.706423  2348 solver.cpp:404]     Test net output #1: loss = 3.72273 (* 1 = 3.72273 loss)
I1122 13:50:48.268450  2348 solver.cpp:228] Iteration 5000, loss = 0.196128
I1122 13:50:48.268486  2348 solver.cpp:244]     Train net output #0: accuracy = 0.9375
I1122 13:50:48.268496  2348 solver.cpp:244]     Train net output #1: loss = 0.196128 (* 1 = 0.196128 loss)
I1122 13:50:48.268503  2348 sgd_solver.cpp:106] Iteration 5000, lr = 0.01
I1122 14:08:57.744341  2348 solver.cpp:337] Iteration 5500, Testing net (#0)
I1122 14:10:11.795549  2348 solver.cpp:404]     Test net output #0: accuracy = 0.335533
I1122 14:10:11.795639  2348 solver.cpp:404]     Test net output #1: loss = 3.6938 (* 1 = 3.6938 loss)
I1122 14:10:12.356119  2348 solver.cpp:228] Iteration 5500, loss = 0.289282
I1122 14:10:12.356159  2348 solver.cpp:244]     Train net output #0: accuracy = 0.914062
I1122 14:10:12.356171  2348 solver.cpp:244]     Train net output #1: loss = 0.289282 (* 1 = 0.289282 loss)
I1122 14:10:12.356181  2348 sgd_solver.cpp:106] Iteration 5500, lr = 0.01
I1122 14:28:21.664803  2348 solver.cpp:337] Iteration 6000, Testing net (#0)
I1122 14:29:35.720373  2348 solver.cpp:404]     Test net output #0: accuracy = 0.3514
I1122 14:29:35.720443  2348 solver.cpp:404]     Test net output #1: loss = 3.66276 (* 1 = 3.66276 loss)
I1122 14:29:36.284900  2348 solver.cpp:228] Iteration 6000, loss = 0.139856
I1122 14:29:36.284936  2348 solver.cpp:244]     Train net output #0: accuracy = 0.96875
I1122 14:29:36.284945  2348 solver.cpp:244]     Train net output #1: loss = 0.139856 (* 1 = 0.139856 loss)
I1122 14:29:36.284952  2348 sgd_solver.cpp:106] Iteration 6000, lr = 0.01
I1122 14:47:45.809798  2348 solver.cpp:337] Iteration 6500, Testing net (#0)
I1122 14:48:59.822242  2348 solver.cpp:404]     Test net output #0: accuracy = 0.345333
I1122 14:48:59.822311  2348 solver.cpp:404]     Test net output #1: loss = 3.78215 (* 1 = 3.78215 loss)
I1122 14:49:00.385233  2348 solver.cpp:228] Iteration 6500, loss = 0.123394
I1122 14:49:00.385262  2348 solver.cpp:244]     Train net output #0: accuracy = 0.976562
I1122 14:49:00.385270  2348 solver.cpp:244]     Train net output #1: loss = 0.123394 (* 1 = 0.123394 loss)
I1122 14:49:00.385277  2348 sgd_solver.cpp:106] Iteration 6500, lr = 0.01
I1122 15:07:09.814561  2348 solver.cpp:337] Iteration 7000, Testing net (#0)
I1122 15:08:23.941373  2348 solver.cpp:404]     Test net output #0: accuracy = 0.357933
I1122 15:08:23.941447  2348 solver.cpp:404]     Test net output #1: loss = 3.73599 (* 1 = 3.73599 loss)
I1122 15:08:24.502104  2348 solver.cpp:228] Iteration 7000, loss = 0.140315
I1122 15:08:24.502140  2348 solver.cpp:244]     Train net output #0: accuracy = 0.945312
I1122 15:08:24.502149  2348 solver.cpp:244]     Train net output #1: loss = 0.140315 (* 1 = 0.140315 loss)
I1122 15:08:24.502156  2348 sgd_solver.cpp:106] Iteration 7000, lr = 0.01
I1122 15:26:33.694986  2348 solver.cpp:337] Iteration 7500, Testing net (#0)
I1122 15:27:47.720224  2348 solver.cpp:404]     Test net output #0: accuracy = 0.350333
I1122 15:27:47.720290  2348 solver.cpp:404]     Test net output #1: loss = 3.66467 (* 1 = 3.66467 loss)
I1122 15:27:48.281172  2348 solver.cpp:228] Iteration 7500, loss = 0.15804
I1122 15:27:48.281205  2348 solver.cpp:244]     Train net output #0: accuracy = 0.929688
I1122 15:27:48.281213  2348 solver.cpp:244]     Train net output #1: loss = 0.15804 (* 1 = 0.15804 loss)
I1122 15:27:48.281219  2348 sgd_solver.cpp:106] Iteration 7500, lr = 0.01
I1122 15:45:57.674273  2348 solver.cpp:337] Iteration 8000, Testing net (#0)
I1122 15:47:11.718900  2348 solver.cpp:404]     Test net output #0: accuracy = 0.3364
I1122 15:47:11.718966  2348 solver.cpp:404]     Test net output #1: loss = 3.84872 (* 1 = 3.84872 loss)
I1122 15:47:12.282114  2348 solver.cpp:228] Iteration 8000, loss = 0.220774
I1122 15:47:12.282147  2348 solver.cpp:244]     Train net output #0: accuracy = 0.929688
I1122 15:47:12.282155  2348 solver.cpp:244]     Train net output #1: loss = 0.220774 (* 1 = 0.220774 loss)
I1122 15:47:12.282166  2348 sgd_solver.cpp:106] Iteration 8000, lr = 0.01
I1122 16:05:21.586694  2348 solver.cpp:337] Iteration 8500, Testing net (#0)
I1122 16:06:35.635238  2348 solver.cpp:404]     Test net output #0: accuracy = 0.346533
I1122 16:06:35.635305  2348 solver.cpp:404]     Test net output #1: loss = 3.70456 (* 1 = 3.70456 loss)
I1122 16:06:36.197615  2348 solver.cpp:228] Iteration 8500, loss = 0.0724085
I1122 16:06:36.197649  2348 solver.cpp:244]     Train net output #0: accuracy = 0.984375
I1122 16:06:36.197657  2348 solver.cpp:244]     Train net output #1: loss = 0.0724085 (* 1 = 0.0724085 loss)
I1122 16:06:36.197664  2348 sgd_solver.cpp:106] Iteration 8500, lr = 0.01
I1122 16:24:45.422049  2348 solver.cpp:337] Iteration 9000, Testing net (#0)
I1122 16:25:59.494092  2348 solver.cpp:404]     Test net output #0: accuracy = 0.351933
I1122 16:25:59.494158  2348 solver.cpp:404]     Test net output #1: loss = 3.87293 (* 1 = 3.87293 loss)
I1122 16:26:00.055619  2348 solver.cpp:228] Iteration 9000, loss = 0.260139
I1122 16:26:00.055645  2348 solver.cpp:244]     Train net output #0: accuracy = 0.9375
I1122 16:26:00.055655  2348 solver.cpp:244]     Train net output #1: loss = 0.260139 (* 1 = 0.260139 loss)
I1122 16:26:00.055661  2348 sgd_solver.cpp:106] Iteration 9000, lr = 0.01
